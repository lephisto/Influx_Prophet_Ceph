{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to achive capacity forecast for Ceph Storage Pools.\n",
    "To make this work you have to:\n",
    "\n",
    "1. Enable Telegraf Telemetry in Ceph:\n",
    "<pre>\n",
    "ceph mgr module enable telegraf\n",
    "ceph telegraf config-set address udp://:8094\n",
    "ceph telegraf config-set interval 10\n",
    "</pre>\n",
    "\n",
    "Additionally you have to Configure your telegraf to forward those broadcasts to your InfluxDB Instance:\n",
    "\n",
    "Create a file like /etc/telegraf/telegraf.d/ceph.conf with the following content:\n",
    "\n",
    "<pre>\n",
    "[[inputs.socket_listener]] \n",
    "  service_address = \"udp://:8094\" \n",
    "  data_format = \"influx\"\n",
    "</pre>\n",
    "\n",
    "\n",
    "2. Adjust the Variables in this script to match your Environment: Adjust the fsid (UUID for the Pool you want to Monitor) and the InfluxDB Credentials\n",
    "\n",
    "3. After tunning this Notebook you should have a new measurement called ceph_cluster_stats_fc for the next 365 days, based on the Data of the past 365 days. You can now easily create a Grafana Dashboard from it.\n",
    "\n",
    "This is just a real-world example on a Ceph Storage Pool. Storage Usage is usually subject to a strong seasonality and therefor a pretty good showcase for timeseries forecasting. Daily and weekly Snapshots + snapshot trimming usually produces a \"heartbeat\" with a high seasonality. It should easily be able to adapt to ZFS or any other Type of Storage.\n",
    "\n",
    "\n",
    "This notebook uses the <a href=\"https://v2.docs.influxdata.com/v2.0/reference/client-libraries/\">Python-InfluxDB Client Library</a> and Facebooks Prophet to make forecasts. The basic approach is derived from the <a href=\"https://github.com/facebook/prophet/blob/master/notebooks/quick_start.ipynb\">quick-start example notebook </a> in the <a href=\"https://github.com/facebook/prophet\">prophet repo</a>.\n",
    "\n",
    "Thanks do anaisdg for providing a basic sample on how to glue InfluxDB with fbProphet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "# Forecast Configuration\n",
    "dst_measurement_name = \"ceph_cluster_stats_fc\"\n",
    "\n",
    "# Ceph Pool Info\n",
    "fsid = \"688287b9-0f7b-4c6b-9f93-ecdadf0af36b\"\n",
    "\n",
    "# Influx Database Credentials\n",
    "url = \"http://odo.fan:8086\"\n",
    "token = \"f'root:root'\"\n",
    "bucket = \"telegraf/autogen\"\n",
    "org = \"-\"\n",
    "client = InfluxDBClient(url=url, token=token, org=org)\n",
    "query_api = client.query_api()\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'from(bucket: \"' + bucket + '\")' \\\n",
    "        '  |> range(start: -365d)' \\\n",
    "        '  |> filter(fn: (r) => r._measurement == \"ceph_cluster_stats\" and r.type_instance == \"data_bytes\" and r.fsid == \"' + fsid + '\")' \\\n",
    "        '  |> aggregateWindow(fn: mean, every: 1h, createEmpty: false)'\n",
    "\n",
    "\n",
    "\n",
    "result = client.query_api().query(org=org, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "for table in result:\n",
    "    for record in table.records:\n",
    "        raw.append((record.get_value(), record.get_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(2256484216484.429, datetime.datetime(2020, 2, 16, 1, 0, tzinfo=tzutc())),\n",
       " (2257008660644.6704, datetime.datetime(2020, 2, 16, 2, 0, tzinfo=tzutc())),\n",
       " (2257599753434.3516, datetime.datetime(2020, 2, 16, 3, 0, tzinfo=tzutc())),\n",
       " (2258339431038.1963, datetime.datetime(2020, 2, 16, 4, 0, tzinfo=tzutc())),\n",
       " (2265435505900.0576, datetime.datetime(2020, 2, 16, 5, 0, tzinfo=tzutc()))]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=== influxdb query into dataframe ===\")\n",
    "print()\n",
    "df=pd.DataFrame(raw, columns=['y','ds'], index=None)\n",
    "df['ds'] = df['ds'].values.astype('<M8[D]')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model by instantiating a new `Prophet` object.  Any settings to the forecasting procedure are passed into the constructor.  Then you call its `fit` method and pass in the historical dataframe. Fitting should take 1-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(weekly_seasonality=True, yearly_seasonality=True,changepoint_prior_scale=0.0001).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are then made on a dataframe with a column `ds` containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method `Prophet.make_future_dataframe`. By default it will also include the dates from the history, so we will see the model fit as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365*24, freq=\"H\")\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method will assign each row in `future` a predicted value which it names `yhat`.  If you pass in historical dates, it will provide an in-sample fit. The `forecast` object here is a new dataframe that includes a column `yhat` with the forecast, as well as columns for components and uncertainty intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['measurement'] = dst_measurement_name\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper','measurement']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the forecast by calling the `Prophet.plot` method and passing in your forecast dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "fig1 = m.plot(forecast)\n",
    "a = add_changepoints_to_plot(fig1.gca(),m,forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper','measurement']].copy()\n",
    "lines = [str(cp[\"measurement\"][d]) \n",
    "         + \",type=forecast\" \n",
    "         + \" \" \n",
    "         + \"yhat=\" + str(cp[\"yhat\"][d]) + \",\"\n",
    "         + \"yhat_lower=\" + str(cp[\"yhat_lower\"][d]) + \",\"\n",
    "         + \"yhat_upper=\" + str(cp[\"yhat_upper\"][d])\n",
    "         + \" \" + str(int(time.mktime(cp['ds'][d].timetuple()))) + \"000000000\" for d in range(len(cp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient, Point, WriteOptions\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "_write_client = client.write_api(write_options=WriteOptions(batch_size=1000, \n",
    "                                                            flush_interval=10000,\n",
    "                                                            jitter_interval=2000,\n",
    "                                                            retry_interval=5000))\n",
    "\n",
    "_write_client.write(bucket, org, lines)\n",
    "\n",
    "lines[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To close client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_write_client.__del__()\n",
    "client.__del__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}