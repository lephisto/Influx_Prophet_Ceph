{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to achive capacity forecast for Ceph Storage Pools.\n",
    "To make this work you have to:\n",
    "\n",
    "1. Enable Telegraf Telemetry in Ceph:\n",
    "<pre>\n",
    "ceph mgr module enable telegraf\n",
    "ceph telegraf config-set address udp://:8094\n",
    "ceph telegraf config-set interval 10\n",
    "</pre>\n",
    "\n",
    "Additionally you have to Configure your telegraf to forward those broadcasts to your InfluxDB Instance:\n",
    "\n",
    "Create a file like /etc/telegraf/telegraf.d/ceph.conf with the following content:\n",
    "\n",
    "<pre>\n",
    "[[inputs.socket_listener]] \n",
    "  service_address = \"udp://:8094\" \n",
    "  data_format = \"influx\"\n",
    "</pre>\n",
    "\n",
    "\n",
    "2. Adjust the Variables in this script to match your Environment: Adjust the fsid (UUID for the Pool you want to Monitor) and the InfluxDB Credentials\n",
    "\n",
    "3. After tunning this Notebook you should have a new measurement called ceph_cluster_stats_fc for the next 365 days, based on the Data of the past 365 days. You can now easily create a Grafana Dashboard from it.\n",
    "\n",
    "This is just a real-world example on a Ceph Storage Pool. Storage Usage is usually subject to a strong seasonality and therefor a pretty good showcase for timeseries forecasting. Daily and weekly Snapshots + snapshot trimming usually produces a \"heartbeat\" with a high seasonality. It should easily be able to adapt to ZFS or any other Type of Storage.\n",
    "\n",
    "\n",
    "This notebook uses the <a href=\"https://v2.docs.influxdata.com/v2.0/reference/client-libraries/\">Python-InfluxDB Client Library</a> and Facebooks Prophet to make forecasts. The basic approach is derived from the <a href=\"https://github.com/facebook/prophet/blob/master/notebooks/quick_start.ipynb\">quick-start example notebook </a> in the <a href=\"https://github.com/facebook/prophet\">prophet repo</a>.\n",
    "\n",
    "4. You migh consider deleting your old forecast when periodically making a new one. \n",
    "\n",
    "```\n",
    "influx delete -o <ORG> --bucket <BUCKET> --predicate '_measurement=\"ceph_cluster_stats_v15_forcast\"' --start '1970-01-01T00:00:00Z' --stop  $(date +\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "```\n",
    "\n",
    "This is not a complete code, more a proof-of-concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from credentials import *\n",
    "\n",
    "# Which measurement to write the forecast to\n",
    "dst_measurement_name = \"ceph_cluster_stats_v15_forcast\"\n",
    "\n",
    "# Ceph Pool fsid\n",
    "fsid = \"acaaae68-7fae-4eab-a389-c729edf8b37e\"\n",
    "\n",
    "# How many days from the past to train?\n",
    "days = 365\n",
    "\n",
    "# Influx Database Credentials\n",
    "url = influx_url\n",
    "token = influx_token\n",
    "bucket = influx_bucket\n",
    "org = influx_org\n",
    "client = InfluxDBClient(url=url, token=token, org=org)\n",
    "query_api = client.query_api()\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'from(bucket: \"' + bucket + '\")' \\\n",
    "        '  |> range(start: -' + days + 'd)' \\\n",
    "        '  |> filter(fn: (r) => r._measurement == \"ceph_cluster_stats_v15\" and r.type_instance == \"bytes_used\" and r.fsid == \"' + fsid + '\")' \\\n",
    "        '  |> aggregateWindow(fn: mean, every: 1h, createEmpty: false)'\n",
    "\n",
    "\n",
    "\n",
    "result = client.query_api().query(org=org, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "for table in result:\n",
    "    for record in table.records:\n",
    "        raw.append((record.get_value(), record.get_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=== influxdb query into dataframe ===\")\n",
    "print()\n",
    "df=pd.DataFrame(raw, columns=['y','ds'], index=None)\n",
    "df['ds'] = df['ds'].values.astype('<M8[D]')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model by instantiating a new `Prophet` object.  Any settings to the forecasting procedure are passed into the constructor.  Then you call its `fit` method and pass in the historical dataframe. Fitting should take 1-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(weekly_seasonality=True, yearly_seasonality=True,changepoint_prior_scale=0.0001).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are then made on a dataframe with a column `ds` containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method `Prophet.make_future_dataframe`. By default it will also include the dates from the history, so we will see the model fit as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=365*24*3, freq=\"H\")\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method will assign each row in `future` a predicted value which it names `yhat`.  If you pass in historical dates, it will provide an in-sample fit. The `forecast` object here is a new dataframe that includes a column `yhat` with the forecast, as well as columns for components and uncertainty intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['measurement'] = dst_measurement_name\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper','measurement']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the forecast by calling the `Prophet.plot` method and passing in your forecast dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet.plot import add_changepoints_to_plot\n",
    "fig1 = m.plot(forecast)\n",
    "a = add_changepoints_to_plot(fig1.gca(),m,forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper','measurement']].copy()\n",
    "lines = [str(cp[\"measurement\"][d]) \n",
    "         + \",type=forecast\" \n",
    "         + \" \" \n",
    "         + \"yhat=\" + str(cp[\"yhat\"][d]) + \",\"\n",
    "         + \"yhat_lower=\" + str(cp[\"yhat_lower\"][d]) + \",\"\n",
    "         + \"yhat_upper=\" + str(cp[\"yhat_upper\"][d])\n",
    "         + \" \" + str(int(time.mktime(cp['ds'][d].timetuple()))) + \"000000000\" for d in range(len(cp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client import InfluxDBClient, Point, WriteOptions\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "\n",
    "_write_client = client.write_api(write_options=WriteOptions(batch_size=1000, \n",
    "                                                            flush_interval=10000,\n",
    "                                                            jitter_interval=2000,\n",
    "                                                            retry_interval=5000))\n",
    "\n",
    "_write_client.write(bucket, org, lines)\n",
    "\n",
    "lines[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To close client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_write_client.__del__()\n",
    "client.__del__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
